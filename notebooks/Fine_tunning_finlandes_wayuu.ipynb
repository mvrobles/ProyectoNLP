{"cells":[{"cell_type":"markdown","metadata":{"id":"9OiwCO8GFZmd"},"source":["# Librerías"]},{"cell_type":"code","source":["!pip install datasets\n","!pip install sacremoses\n","!pip install sacrebleu\n","!pip install evaluate\n","!pip install transformers[sentencepiece]\n","!pip install transformers[torch]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2ozHyTMFyVz","executionInfo":{"status":"ok","timestamp":1700846985567,"user_tz":300,"elapsed":53347,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"}},"outputId":"c059c66f-04f7-46e0-8af6-c2997ec9f420"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n","Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n","Collecting sacremoses\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n","Installing collected packages: sacremoses\n","Successfully installed sacremoses-0.1.1\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m858.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.3.2\n","Collecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.15.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: responses, evaluate\n","Successfully installed evaluate-0.4.1 responses-0.18.0\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n","Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n","Installing collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n","Collecting accelerate>=0.20.3 (from transformers[torch])\n","  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.24.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PbAfhd2sFZmg","executionInfo":{"status":"ok","timestamp":1700846985567,"user_tz":300,"elapsed":7,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tqdm\n","import sys\n","import os"]},{"cell_type":"code","source":["from datasets import load_dataset, DatasetDict, Dataset\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","import pandas as pd"],"metadata":{"id":"JIy4L2ikGsaa","executionInfo":{"status":"ok","timestamp":1700846993534,"user_tz":300,"elapsed":7972,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from transformers import Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq\n","from transformers import AutoModelForSeq2SeqLM\n","from transformers import EarlyStoppingCallback\n","from transformers import Seq2SeqTrainer\n","\n","import numpy as np\n","import pickle\n","import evaluate"],"metadata":{"id":"ZJZZvjzCGuUo","executionInfo":{"status":"ok","timestamp":1700847220676,"user_tz":300,"elapsed":2,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09xx65lPFgOs","executionInfo":{"status":"ok","timestamp":1700847026843,"user_tz":300,"elapsed":29069,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"}},"outputId":"9c8df091-a554-411f-be7e-109844d33e92"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Funciones auxiliares"],"metadata":{"id":"sOwd1XDMGgNv"}},{"cell_type":"code","source":["def preprocess_dataset(path_dataset: str, lang_output: str):\n","  \"\"\"\n","  Lee los datos y los preprocesa. Lo pasa al formato necesario DatasetDict\n","  y divide los datos en train, test y validación.\n","  Sirve para traducción de indígena a español\n","\n","  input:\n","  - path_dataset: con la ruta en donde se encuentra la base a procesar\n","  - lang_output: wayuu, arh de donde va a terminar la traducción\n","\n","  output:\n","  - dataset_dict: DatasetDict con train test y validation\n","  \"\"\"\n","  # Lectura de datos y conversión a diccionario\n","  dataset = pd.read_csv(path_dataset)\n","  conv = {'esp': 'es', 'wayuu': lang_output, 'arh': lang_output}\n","  dataset.rename(columns = conv, inplace = True)\n","\n","  dataset = [{'es': row['es'], lang_output: row[lang_output]} for _, row in dataset.iterrows()]\n","\n","  # División train, test y validación\n","  train, test = train_test_split(dataset, test_size = 0.2, random_state = 42)\n","  val, test = train_test_split(test, test_size = 0.5, random_state = 42)\n","\n","  # Creación de datasets\n","  train = Dataset.from_dict({\"id\": list(range(len(train))), \"translation\": train})\n","  test = Dataset.from_dict({\"id\": list(range(len(test))), \"translation\": test})\n","  validation = Dataset.from_dict({\"id\": list(range(len(val))), \"translation\": val})\n","\n","  # Creación del diccionario\n","  dataset_dict = DatasetDict({\"train\": train, \"test\": test, \"validation\": validation})\n","\n","  return dataset_dict\n","\n","def tokenizar(dataset_dict, model_checkpoint, max_length = 150):\n","  \"\"\"\n","  A partir de un DatasetDict, tokeniza los datos. Esto depende del modelo a utilizar,\n","  y de un modelo específico.\n","\n","  input:\n","  - dataset_dict: con los datos de train, test y validación\n","  - model_checkpoint: identificador del modelo a utilizar\n","  - max_length: de las sentencias a considerar\n","\n","  output:\n","  - tokenized_datasets\n","  \"\"\"\n","  # Cargar tokenizador\n","  tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n","\n","  def preprocess_function(examples):\n","      inputs = [ex[\"es\"] for ex in examples[\"translation\"]]\n","      targets = [ex[\"fi\"] for ex in examples[\"translation\"]]\n","      model_inputs = tokenizer(\n","          inputs, text_target=targets, max_length=max_length, truncation=True\n","      )\n","      return model_inputs\n","\n","  # Tokenizar los datos\n","  tokenized_datasets = dataset_dict.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=dataset_dict[\"train\"].column_names,\n","  )\n","\n","  return tokenized_datasets, tokenizer"],"metadata":{"id":"vfWUzDLoGnBa","executionInfo":{"status":"ok","timestamp":1700848794855,"user_tz":300,"elapsed":370,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def get_model(tokenized_datasets, tokenizer, model_checkpoint,\n","              learning_rate = 2e-5, epochs = 3, weight_decay = 0.01):\n","\n","  model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","  data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","  metric = evaluate.load(\"sacrebleu\")\n","\n","  def compute_metrics(eval_preds):\n","      preds, labels = eval_preds\n","      # In case the model returns more than the prediction logits\n","      if isinstance(preds, tuple):\n","          preds = preds[0]\n","\n","      decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","      # Replace -100s in the labels as we can't decode them\n","      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","      # Some simple post-processing\n","      decoded_preds = [pred.strip() for pred in decoded_preds]\n","      decoded_labels = [[label.strip()] for label in decoded_labels]\n","\n","      result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","      return {\"bleu\": result[\"score\"]}\n","\n","  args = Seq2SeqTrainingArguments(\n","      f\"marian-finetuned-kde4-es-to-fi\",\n","      evaluation_strategy= \"no\",\n","      save_strategy=\"no\", # \"epoch\"\n","      learning_rate = learning_rate,\n","      per_device_train_batch_size=32,\n","      per_device_eval_batch_size=64,\n","      weight_decay=weight_decay,\n","      save_total_limit=3,\n","      num_train_epochs=epochs,\n","      predict_with_generate=True,\n","      fp16=True,\n","      push_to_hub=False,\n","      load_best_model_at_end = True\n","  )\n","\n","  early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0)\n","\n","  trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n","    #callbacks=[early_stopping_callback]\n","  )\n","\n","  return trainer"],"metadata":{"id":"8aZvek_eGhbN","executionInfo":{"status":"ok","timestamp":1700848861257,"user_tz":300,"elapsed":2,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EBUV9QjKFZmh"},"source":["# Parametros"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"IARsmQdyFZmh","executionInfo":{"status":"ok","timestamp":1700848861972,"user_tz":300,"elapsed":4,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"}}},"outputs":[],"source":["path = '/content/drive/MyDrive/Colab Notebooks/Talleres NLP/Proyecto/data_clean'\n","path_out = '/content/drive/MyDrive/Colab Notebooks/Talleres NLP/Proyecto/results'\n","\n","params = {\n","    'dataset': ['/wayuu/COMP_ND.csv', '/wayuu/COMP_NDU.csv', '/wayuu/COMP_NC.csv', '/wayuu/COMP.csv'],\n","    'epochs': [3, 5, 10],\n","    'learning_rate' : [2e-5, 2e-4]\n","}\n","\n","model_checkpoint = 'Helsinki-NLP/opus-mt-es-fi'\n","\n","max_length = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZ9o6PIjFZmi"},"outputs":[],"source":["for d in params['dataset']:\n","\n","  # Procesar los datos\n","  dataset_dict = preprocess_dataset(path + d, lang_output = 'fi')\n","  tokenized_dataset, tokenizer = tokenizar(dataset_dict, model_checkpoint)\n","\n","  for e in params['epochs']:\n","      for lr in params['learning_rate']:\n","\n","          # Crear el modelo\n","          trainer = get_model(tokenized_dataset, tokenizer, model_checkpoint,\n","            learning_rate = lr, epochs = e)\n","\n","          # Nombre-datos\n","          d = d.split('/')[-1].split('.')[0]\n","\n","          # Obtener métricas antes\n","          #metrics1 = trainer.evaluate(max_length = max_length)\n","          #with open(path_out + f'/metrica_antes_{d}_{e}_{lr}.pickle', 'wb') as file:\n","          #  pickle.dump(metrics1, file)\n","\n","          #print(metrics1)\n","\n","          # Entrenar\n","          trainer.train()\n","          trainer.save_model(path_out + f'/modelo_{d}_{e}_{lr}')\n","\n","          # Obtener métricas después\n","          metrics2 = trainer.evaluate(max_length = max_length)\n","          with open(path_out + f'/metrica_despues_{d}_{e}_{lr}.pickle', 'wb') as file:\n","            pickle.dump(metrics2, file)\n","\n","          print(metrics2)"]},{"cell_type":"code","source":[],"metadata":{"id":"jtMTPDluZREm"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}