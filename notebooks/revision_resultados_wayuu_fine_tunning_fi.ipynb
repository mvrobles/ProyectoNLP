{"cells":[{"cell_type":"markdown","metadata":{"id":"p5zWSWGSMvxO"},"source":["# Librerías"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57353,"status":"ok","timestamp":1701050326492,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"0YGgNoXWMsGJ","outputId":"db5b96f9-237d-4a57-bde0-047b2880afd6"},"outputs":[],"source":["# !pip install datasets\n","# !pip install sacremoses\n","# !pip install sacrebleu\n","# !pip install evaluate\n","# !pip install transformers[sentencepiece]\n","# !pip install transformers[torch]"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":294,"status":"ok","timestamp":1701050326782,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"_bVY1wDaM7zd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/melissa/miniconda3/envs/env_NLP/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from glob import glob\n","import pandas as pd\n","import numpy as np\n","from tqdm.auto import tqdm, trange\n","import sys\n","import os"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10262,"status":"ok","timestamp":1701050337040,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"_eDIoAzqM-r2"},"outputs":[],"source":["from datasets import load_dataset, DatasetDict, Dataset\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5434,"status":"ok","timestamp":1701050342452,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"Tc19GgDmM_J2"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","from transformers import DataCollatorForSeq2Seq\n","from transformers import AutoModelForSeq2SeqLM\n","from transformers import EarlyStoppingCallback\n","from transformers import Seq2SeqTrainer\n","\n","import torch\n","\n","import numpy as np\n","import pickle\n","import evaluate"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19292,"status":"ok","timestamp":1701050361740,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"dgJwKMnONBjs","outputId":"2d7d372e-0bca-4689-d1c6-728ea143029d"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import sacrebleu\n","bleu_calc = sacrebleu.BLEU()\n","chrf_calc = sacrebleu.CHRF(word_order=2)"]},{"cell_type":"markdown","metadata":{},"source":["# Funciones auxiliares "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def preprocess_dataset(path_dataset: str, lang_output: str):\n","  \"\"\"\n","  Lee los datos y los preprocesa. Lo pasa al formato necesario DatasetDict\n","  y divide los datos en train, test y validación.\n","  Sirve para traducción de indígena a español\n","\n","  input:\n","  - path_dataset: con la ruta en donde se encuentra la base a procesar\n","  - lang_output: wayuu, arh de donde va a terminar la traducción\n","\n","  output:\n","  - dataset_dict: DatasetDict con train test y validation\n","  \"\"\"\n","  # Lectura de datos y conversión a diccionario\n","  dataset = pd.read_csv(path_dataset)\n","  conv = {'esp': 'es', 'wayuu': lang_output, 'arh': lang_output}\n","  dataset.rename(columns = conv, inplace = True)\n","\n","  dataset = [{'es': row['es'], lang_output: row[lang_output]} for _, row in dataset.iterrows()]\n","\n","  # División train, test y validación\n","  train, test = train_test_split(dataset, test_size = 0.2, random_state = 42)\n","  val, test = train_test_split(test, test_size = 0.5, random_state = 42)\n","\n","  # Creación de datasets\n","  train = Dataset.from_dict({\"id\": list(range(len(train))), \"translation\": train})\n","  test = Dataset.from_dict({\"id\": list(range(len(test))), \"translation\": test})\n","  validation = Dataset.from_dict({\"id\": list(range(len(val))), \"translation\": val})\n","\n","  # Creación del diccionario\n","  dataset_dict = DatasetDict({\"train\": train, \"test\": test, \"validation\": validation})\n","\n","  return dataset_dict"]},{"cell_type":"markdown","metadata":{"id":"c5g1scd6NDJX"},"source":["# "]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1701050361740,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"9Qko-jxgNB_p"},"outputs":[],"source":["def tokenizar(dataset_dict, tokenizer, max_length = 150):\n","  \"\"\"\n","  A partir de un DatasetDict, tokeniza los datos. Esto depende del modelo a utilizar,\n","  y de un modelo específico.\n","\n","  input:\n","  - dataset_dict: con los datos de train, test y validación\n","  - tokenizer: tokenizer\n","  - max_length: de las sentencias a considerar\n","\n","  output:\n","  - tokenized_datasets\n","  \"\"\"\n","\n","  def preprocess_function(examples):\n","      inputs = [ex[\"es\"] for ex in examples[\"translation\"]]\n","      targets = [ex[\"fi\"] for ex in examples[\"translation\"]]\n","      model_inputs = tokenizer(\n","          inputs, text_target=targets, max_length=max_length, truncation=True\n","      )\n","      return model_inputs\n","\n","  # Tokenizar los datos\n","  tokenized_datasets = dataset_dict.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=dataset_dict[\"train\"].column_names,\n","  )\n","\n","  return tokenized_datasets, tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluamos el mejor modelo en cuanto a BLEU en entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"0EVrGdKMftik"},"source":["Veamos el modelo que dio mejor métrica BLUE en el entrenamiento para validation."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":202,"status":"ok","timestamp":1701050499392,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"szbOQ4W6fyVa"},"outputs":[],"source":["model_path = \"../results/wayuu\"\n","eval_blues = {}\n","\n","for res in glob(model_path + '/*'):\n","  if 'pickle' in res and 'resultados' not in res:\n","    with open(res, 'rb') as file:\n","      blue_score = pickle.load(file)['eval_bleu']\n","      eval_blues[res] = blue_score"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["{'../results/wayuu/metrica_despues_COMP_5_0.0002.pickle': 8.46147848277743,\n"," '../results/wayuu/metrica_despues_COMP_NDU_10_2e-05.pickle': 3.823045364057283,\n"," '../results/wayuu/metrica_despues_COMP_NDU_5_0.0002.pickle': 8.358963761551946,\n"," '../results/wayuu/metrica_despues_COMP_10_2e-05.pickle': 3.841921542846619,\n"," '../results/wayuu/metrica_despues_COMP_ND_3_0.0002.pickle': 1.677752319229074,\n"," '../results/wayuu/metrica_despues_COMP_NC_3_0.0002.pickle': 6.985037271297072,\n"," '../results/wayuu/metrica_despues_COMP_10_0.0002.pickle': 9.944518207708816,\n"," '../results/wayuu/metrica_despues_COMP_ND_10_0.0002.pickle': 6.114102976300334,\n"," '../results/wayuu/metrica_despues_COMP_ND_5_2e-05.pickle': 0.3364212705823903,\n"," '../results/wayuu/metrica_despues_COMP_ND_5_0.0002.pickle': 3.3616074258638515,\n"," '../results/wayuu/metrica_despues_COMP_5_2e-05.pickle': 2.0283968999388584,\n"," '../results/wayuu/metrica_despues_COMP_NC_10_0.0002.pickle': 10.092185362846715,\n"," '../results/wayuu/metrica_despues_COMP_NC_5_2e-05.pickle': 1.9693512837568947,\n"," '../results/wayuu/metrica_despues_COMP_NDU_3_0.0002.pickle': 6.868830688240474,\n"," '../results/wayuu/metrica_despues_COMP_3_2e-05.pickle': 1.1123616356168728,\n"," '../results/wayuu/metrica_despues_COMP_ND_3_2e-05.pickle': 0.06467012274343785,\n"," '../results/wayuu/metrica_despues_COMP_NC_5_0.0002.pickle': 8.630473080537001,\n"," '../results/wayuu/metrica_despues_COMP_NC_3_2e-05.pickle': 1.0976952286993389,\n"," '../results/wayuu/metrica_despues_COMP_NC_10_2e-05.pickle': 3.8415706127918066,\n"," '../results/wayuu/metrica_despues_COMP_NDU_3_2e-05.pickle': 1.1178175632332183,\n"," '../results/wayuu/metrica_despues_COMP_3_0.0002.pickle': 6.903353158787755,\n"," '../results/wayuu/metrica_despues_COMP_NDU_5_2e-05.pickle': 2.057857258257621,\n"," '../results/wayuu/metrica_despues_COMP_NDU_10_0.0002.pickle': 9.837815383461848,\n"," '../results/wayuu/metrica_despues_COMP_ND_10_2e-05.pickle': 0.768654818512039}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["eval_blues"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701050557958,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"ls6TprGFgtHx"},"outputs":[],"source":["res_max = max(eval_blues, key=lambda k: eval_blues[k])"]},{"cell_type":"markdown","metadata":{"id":"LR1YbQqllnVg"},"source":["Este es el modelo con mejor score BLUE para el conjunto de validación entre los entrenados"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1701050567422,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"nXDxndcHg8Hb","outputId":"917f50b6-44cf-46aa-91ee-51bf9581e38e"},"outputs":[{"data":{"text/plain":["('../results/wayuu/metrica_despues_COMP_NC_10_0.0002.pickle',\n"," 10.092185362846715)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["res_max, eval_blues[res_max]"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":5078,"status":"ok","timestamp":1701051054857,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"c-uRU8owPIqh"},"outputs":[{"name":"stdout","output_type":"stream","text":["COMP_NC\n"]}],"source":["path_data = '../data/wayuu'\n","\n","d = '_'.join(res_max.split('_')[-4:-2])\n","print(d)\n","\n","# Cargar datos\n","dataset_dict = preprocess_dataset(path_data + '/' + d + '.csv', lang_output = 'fi')\n","\n","# Cargar modelo y tokenizados\n","name = res_max.split('.pickle')[0].replace('metrica_despues', 'modelo')\n","tokenizer = AutoTokenizer.from_pretrained(name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(name)\n","\n","df_test = pd.DataFrame(dataset_dict['test']['translation'])\n","df_train = pd.DataFrame(dataset_dict['train']['translation'])\n","df_validation = pd.DataFrame(dataset_dict['validation']['translation'])"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def translate(text, src_lang='es', tgt_lang='fi', a=32, b=3, max_input_length=128, num_beams=4, **kwargs):\n","    tokenizer.src_lang = src_lang\n","    tokenizer.tgt_lang = tgt_lang\n","    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_input_length)\n","    result = model.generate(\n","        **inputs.to(model.device),\n","        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n","        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n","        num_beams=num_beams,\n","        **kwargs\n","    )\n","    return tokenizer.batch_decode(result, skip_special_tokens=True)\n","\n","def batched_translate(texts, batch_size=16, **kwargs):\n","    \"\"\"Translate texts in batches of similar length\"\"\"\n","    idxs, texts2 = zip(*sorted(enumerate(texts), key=lambda p: len(p[1]), reverse=True))\n","    results = []\n","    for i in trange(0, len(texts2), batch_size):\n","        results.extend(translate(texts2[i: i+batch_size], **kwargs))\n","    return [p for i, p in sorted(zip(idxs, results))]"]},{"cell_type":"markdown","metadata":{"id":"ZU-pKFmBluHf"},"source":["Obtenemos las predicciones"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>es</th>\n","      <th>fi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>renunciamos a actuar de forma oculta y avergon...</td>\n","      <td>tu tekirajakat anain jia, nnojotsu taattajaain...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>si el mundo los odia a ustedes, sepan que prim...</td>\n","      <td>aashajaashi jesus nuchikua namuin   muleka kan...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>como podemos ser mas compasivos</td>\n","      <td>kasa waa'inrajatka supula alinjatuin wayuu wapula</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>jesus acababa de ense arles a sus discipulos q...</td>\n","      <td>ni'ikuin jesuu na nikirajuinkana sunain achunt...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>con que se divierten hoy dia muchas personas</td>\n","      <td>kasa naainjaka ma'in na wayuukana maa'ulu yaa</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8124</th>\n","      <td>ma ana vere a mis hermanos</td>\n","      <td>tereena huata tawalayu</td>\n","    </tr>\n","    <tr>\n","      <th>8125</th>\n","      <td>pero es preciso que sean constantes en el cump...</td>\n","      <td>anakaja nnojorule juu'ulaain suulia anoujaa. m...</td>\n","    </tr>\n","    <tr>\n","      <th>8126</th>\n","      <td>pero poco a poco sus sentimientos se haran mas...</td>\n","      <td>mapa ki'raleeshi'iya naya sunain muin nakuwa'ipa</td>\n","    </tr>\n","    <tr>\n","      <th>8127</th>\n","      <td>sin embargo eso no quiere decir que no tengamo...</td>\n","      <td>kamaneepaja'a waya sumuin wayuu supushua'a</td>\n","    </tr>\n","    <tr>\n","      <th>8128</th>\n","      <td>el esta mintiendo</td>\n","      <td>niakai akujushi na'alain</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8129 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                     es  \\\n","0     renunciamos a actuar de forma oculta y avergon...   \n","1     si el mundo los odia a ustedes, sepan que prim...   \n","2                       como podemos ser mas compasivos   \n","3     jesus acababa de ense arles a sus discipulos q...   \n","4          con que se divierten hoy dia muchas personas   \n","...                                                 ...   \n","8124                         ma ana vere a mis hermanos   \n","8125  pero es preciso que sean constantes en el cump...   \n","8126  pero poco a poco sus sentimientos se haran mas...   \n","8127  sin embargo eso no quiere decir que no tengamo...   \n","8128                                  el esta mintiendo   \n","\n","                                                     fi  \n","0     tu tekirajakat anain jia, nnojotsu taattajaain...  \n","1     aashajaashi jesus nuchikua namuin   muleka kan...  \n","2     kasa waa'inrajatka supula alinjatuin wayuu wapula  \n","3     ni'ikuin jesuu na nikirajuinkana sunain achunt...  \n","4         kasa naainjaka ma'in na wayuukana maa'ulu yaa  \n","...                                                 ...  \n","8124                             tereena huata tawalayu  \n","8125  anakaja nnojorule juu'ulaain suulia anoujaa. m...  \n","8126   mapa ki'raleeshi'iya naya sunain muin nakuwa'ipa  \n","8127         kamaneepaja'a waya sumuin wayuu supushua'a  \n","8128                           niakai akujushi na'alain  \n","\n","[8129 rows x 2 columns]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["df_test"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["fce4c7afab9e47a0a3a401593e9c97e8","0e45617308c04c558e54164d106f5f30","be70aae8797b487f9d9f7d56fd0875db","43479b3a025d44b087ea8dee0e5a57f8","0c91cafc7e9a4d588d51259c9a828816","9788c66bd5ee4c0f95ae8dbbccc324e5","99ec65fa7dc6436f9a5466abb526dc35","60abb1a1b1e54f29a5ca1c66a82996e6","a7ac5cf37e2649888c6b2521ddd149e7","f691bef9bb9a49889e8109837a9c2e3a","fc4adf33d874464e97ce682b8a38920a"]},"executionInfo":{"elapsed":1120457,"status":"ok","timestamp":1701052190372,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"2rQAc7kEiwRh","outputId":"b1022c84-1cd5-44e8-bf59-c451f6fdce94"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 8129/8129 [2:58:14<00:00,  1.32s/it]  \n"]}],"source":["wayuu_pred = []\n","for i in trange(0, len(df_test.values)):\n","    translated = translate(df_test.es[i])\n","    wayuu_pred.append(translated)"]},{"cell_type":"markdown","metadata":{"id":"MoKyq7YSly37"},"source":["# Métricas en test"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701052272920,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"zHXJ1o_InbDU","outputId":"fc33d3d1-8e9b-4f77-d998-6c6e4c448502"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>es</th>\n","      <th>fi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>renunciamos a actuar de forma oculta y avergon...</td>\n","      <td>tu tekirajakat anain jia, nnojotsu taattajaain...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>si el mundo los odia a ustedes, sepan que prim...</td>\n","      <td>aashajaashi jesus nuchikua namuin   muleka kan...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>como podemos ser mas compasivos</td>\n","      <td>kasa waa'inrajatka supula alinjatuin wayuu wapula</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>jesus acababa de ense arles a sus discipulos q...</td>\n","      <td>ni'ikuin jesuu na nikirajuinkana sunain achunt...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>con que se divierten hoy dia muchas personas</td>\n","      <td>kasa naainjaka ma'in na wayuukana maa'ulu yaa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  es  \\\n","0  renunciamos a actuar de forma oculta y avergon...   \n","1  si el mundo los odia a ustedes, sepan que prim...   \n","2                    como podemos ser mas compasivos   \n","3  jesus acababa de ense arles a sus discipulos q...   \n","4       con que se divierten hoy dia muchas personas   \n","\n","                                                  fi  \n","0  tu tekirajakat anain jia, nnojotsu taattajaain...  \n","1  aashajaashi jesus nuchikua namuin   muleka kan...  \n","2  kasa waa'inrajatka supula alinjatuin wayuu wapula  \n","3  ni'ikuin jesuu na nikirajuinkana sunain achunt...  \n","4      kasa naainjaka ma'in na wayuukana maa'ulu yaa  "]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["df_test.head()"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701052285038,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"2vvAmk_WljuU"},"outputs":[],"source":["import sacrebleu\n","bleu_calc = sacrebleu.BLEU()\n","chrf_calc = sacrebleu.CHRF(word_order=2)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["wayuu_pred2 = [w[0] for w in wayuu_pred] "]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701052287536,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"gWBKJWxqerDR","outputId":"507a827c-7487-48ca-ff0a-55b559ead24e"},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU = 7.48 31.0/12.0/5.6/2.9 (BP = 0.849 ratio = 0.859 hyp_len = 95933 ref_len = 111620)\n","chrF2++ = 31.90\n"]}],"source":["print(bleu_calc.corpus_score(wayuu_pred2, [df_test['fi'].tolist()]))\n","print(chrf_calc.corpus_score(wayuu_pred2, [df_test['fi'].tolist()]))"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":230,"status":"ok","timestamp":1701052306889,"user":{"displayName":"Melissa Robles","userId":"13164412100565085494"},"user_tz":300},"id":"syo5p9ebjnSQ"},"outputs":[],"source":["with open(model_path + '/resultados_traducciones_mejor_modelo_test_wayuu.pickle', 'wb') as file:\n","  pickle.dump(wayuu_pred, file)"]},{"cell_type":"markdown","metadata":{},"source":["# Revisión otros modelos - No diccionario"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["['../results/wayuu/metrica_despues_COMP_ND_3_0.0002.pickle',\n"," '../results/wayuu/metrica_despues_COMP_ND_10_0.0002.pickle',\n"," '../results/wayuu/metrica_despues_COMP_ND_5_2e-05.pickle',\n"," '../results/wayuu/metrica_despues_COMP_ND_5_0.0002.pickle',\n"," '../results/wayuu/metrica_despues_COMP_ND_3_2e-05.pickle',\n"," '../results/wayuu/metrica_despues_COMP_ND_10_2e-05.pickle']"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["keys_nd = [k for k in eval_blues.keys() if 'ND_' in k]\n","keys_nd"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["path_data = '../data/wayuu'\n","\n","d = 'COMP_ND'\n","\n","# Cargar datos\n","dataset_dict = preprocess_dataset(path_data + '/' + d + '.csv', lang_output = 'fi')\n","\n","df_test = pd.DataFrame(dataset_dict['test']['translation'])\n","df_train = pd.DataFrame(dataset_dict['train']['translation'])\n","df_validation = pd.DataFrame(dataset_dict['validation']['translation'])"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 894/894 [35:55<00:00,  2.41s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_ND_3_0.0002.pickle\n","\n","BLEU = 0.68 28.6/5.3/1.1/0.2 (BP = 0.281 ratio = 0.441 hyp_len = 22764 ref_len = 51637)\n","chrF2++ = 15.47\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 894/894 [37:26<00:00,  2.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_ND_10_0.0002.pickle\n","\n","BLEU = 2.53 36.6/10.9/4.1/1.8 (BP = 0.342 ratio = 0.482 hyp_len = 24907 ref_len = 51637)\n","chrF2++ = 21.37\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 894/894 [51:29<00:00,  3.46s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_ND_5_2e-05.pickle\n","\n","BLEU = 0.17 8.1/0.9/0.1/0.0 (BP = 0.641 ratio = 0.692 hyp_len = 35726 ref_len = 51637)\n","chrF2++ = 9.15\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 894/894 [37:06<00:00,  2.49s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_ND_5_0.0002.pickle\n","\n","BLEU = 1.47 31.7/7.7/2.3/0.7 (BP = 0.327 ratio = 0.472 hyp_len = 24381 ref_len = 51637)\n","chrF2++ = 18.49\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 894/894 [50:45<00:00,  3.41s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_ND_3_2e-05.pickle\n","\n","BLEU = 0.08 3.9/0.2/0.0/0.0 (BP = 0.610 ratio = 0.669 hyp_len = 34544 ref_len = 51637)\n","chrF2++ = 7.55\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 894/894 [37:25<00:00,  2.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_ND_10_2e-05.pickle\n","\n","BLEU = 0.37 20.5/3.0/0.4/0.0 (BP = 0.348 ratio = 0.487 hyp_len = 25127 ref_len = 51637)\n","chrF2++ = 12.79\n"]}],"source":["for model_name in keys_nd:\n","    # Cargar modelo y tokenizados\n","    name = model_name.split('.pickle')[0].replace('metrica_despues', 'modelo')\n","    tokenizer = AutoTokenizer.from_pretrained(name)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(name)\n","\n","    # Traducir\n","    print('Comienza a predecir')\n","    wayuu_pred = []\n","    for i in trange(0, len(df_test.values)):\n","        translated = translate(df_test.es[i])\n","        wayuu_pred.append(translated)\n","    wayuu_pred2 = [w[0] for w in wayuu_pred] \n","\n","    # Resultados \n","    print(f'\\n\\n ---------------------- Resultados {model_name}\\n')\n","    print(bleu_calc.corpus_score(wayuu_pred2, [df_test['fi'].tolist()]))\n","    print(chrf_calc.corpus_score(wayuu_pred2, [df_test['fi'].tolist()]))\n","\n","    # Guardar \n","    name = name.split('/')[-1]\n","    with open(model_path + '/resultados_traducciones_{name}_wayuu.pickle', 'wb') as file:\n","        pickle.dump(wayuu_pred, file)"]},{"cell_type":"markdown","metadata":{},"source":["# Revisión otros modelos - No diccionario palabras únicas"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["COMP_NDU\n"]}],"source":["path_data = '../data/wayuu'\n","\n","d = 'COMP_NDU'\n","print(d)\n","\n","# Cargar datos\n","dataset_dict = preprocess_dataset(path_data + '/' + d + '.csv', lang_output = 'fi')\n","\n","df_test = pd.DataFrame(dataset_dict['test']['translation'])\n","df_train = pd.DataFrame(dataset_dict['train']['translation'])\n","df_validation = pd.DataFrame(dataset_dict['validation']['translation'])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["['../results/wayuu/metrica_despues_COMP_NDU_10_2e-05.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NDU_5_0.0002.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NDU_3_0.0002.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NDU_3_2e-05.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NDU_5_2e-05.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NDU_10_0.0002.pickle']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["keys_nd = [k for k in eval_blues.keys() if 'NDU_' in k]\n","keys_nd"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7925/7925 [4:48:02<00:00,  2.18s/it]     \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NDU_10_2e-05.pickle\n","\n","BLEU = 2.65 24.2/6.2/1.8/0.5 (BP = 0.769 ratio = 0.792 hyp_len = 86944 ref_len = 109834)\n","chrF2++ = 24.93\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7925/7925 [2:44:16<00:00,  1.24s/it]     \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NDU_5_0.0002.pickle\n","\n","BLEU = 6.47 30.4/11.1/4.8/2.3 (BP = 0.832 ratio = 0.845 hyp_len = 92780 ref_len = 109834)\n","chrF2++ = 30.82\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7925/7925 [6:44:44<00:00,  3.06s/it]     \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NDU_3_0.0002.pickle\n","\n","BLEU = 5.33 28.5/9.7/3.8/1.6 (BP = 0.824 ratio = 0.838 hyp_len = 92056 ref_len = 109834)\n","chrF2++ = 29.27\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7925/7925 [3:05:25<00:00,  1.40s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NDU_3_2e-05.pickle\n","\n","BLEU = 0.89 17.6/2.7/0.4/0.1 (BP = 0.791 ratio = 0.810 hyp_len = 88985 ref_len = 109834)\n","chrF2++ = 20.20\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7925/7925 [5:51:16<00:00,  2.66s/it]     \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NDU_5_2e-05.pickle\n","\n","BLEU = 1.49 20.6/4.0/0.8/0.2 (BP = 0.775 ratio = 0.797 hyp_len = 87567 ref_len = 109834)\n","chrF2++ = 22.22\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7925/7925 [5:09:23<00:00,  2.34s/it]    \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NDU_10_0.0002.pickle\n","\n","BLEU = 7.53 32.1/12.4/5.8/3.0 (BP = 0.826 ratio = 0.839 hyp_len = 92163 ref_len = 109834)\n","chrF2++ = 31.98\n"]}],"source":["for model_name in keys_nd:\n","    # Cargar modelo y tokenizados\n","    name = model_name.split('.pickle')[0].replace('metrica_despues', 'modelo')\n","    tokenizer = AutoTokenizer.from_pretrained(name)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(name)\n","\n","    # Traducir\n","    print('Comienza a predecir')\n","    wayuu_pred = []\n","    for i in trange(0, len(df_test.values)):\n","        translated = translate(df_test.es[i])\n","        wayuu_pred.append(translated)\n","    wayuu_pred2 = [w[0] for w in wayuu_pred] \n","\n","    # Resultados \n","    print(f'\\n\\n ---------------------- Resultados {model_name}\\n')\n","    print(bleu_calc.corpus_score(wayuu_pred2, [df_test['fi'].tolist()]))\n","    print(chrf_calc.corpus_score(wayuu_pred2, [df_test['fi'].tolist()]))\n","\n","    # Guardar \n","    name = name.split('/')[-1]\n","    with open(model_path + '/resultados_traducciones_{name}_wayuu.pickle', 'wb') as file:\n","        pickle.dump(wayuu_pred, file)"]},{"cell_type":"markdown","metadata":{},"source":["# Revisión otros modelos - No constitución"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["COMP_NC\n"]}],"source":["path_data = '../data/wayuu'\n","\n","d = 'COMP_NC'\n","print(d)\n","\n","# Cargar datos\n","dataset_dict = preprocess_dataset(path_data + '/' + d + '.csv', lang_output = 'fi')\n","\n","df_test = pd.DataFrame(dataset_dict['test']['translation'])\n","df_train = pd.DataFrame(dataset_dict['train']['translation'])\n","df_validation = pd.DataFrame(dataset_dict['validation']['translation'])"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["['../results/wayuu/metrica_despues_COMP_NC_3_0.0002.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NC_10_0.0002.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NC_5_2e-05.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NC_5_0.0002.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NC_3_2e-05.pickle',\n"," '../results/wayuu/metrica_despues_COMP_NC_10_2e-05.pickle']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["keys_nc = [k for k in eval_blues.keys() if 'NC_' in k]\n","keys_nc"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8129/8129 [2:22:56<00:00,  1.06s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NC_3_0.0002.pickle\n","\n","BLEU = 5.02 28.8/9.7/3.8/1.6 (BP = 0.777 ratio = 0.799 hyp_len = 89152 ref_len = 111620)\n","chrF2++ = 28.88\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8129/8129 [2:22:13<00:00,  1.05s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NC_10_0.0002.pickle\n","\n","BLEU = 7.48 31.0/12.0/5.6/2.9 (BP = 0.849 ratio = 0.859 hyp_len = 95933 ref_len = 111620)\n","chrF2++ = 31.90\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8129/8129 [2:35:50<00:00,  1.15s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NC_5_2e-05.pickle\n","\n","BLEU = 1.49 20.8/4.1/0.9/0.2 (BP = 0.739 ratio = 0.768 hyp_len = 85680 ref_len = 111620)\n","chrF2++ = 21.88\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8129/8129 [2:23:39<00:00,  1.06s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NC_5_0.0002.pickle\n","\n","BLEU = 6.26 29.6/10.8/4.7/2.1 (BP = 0.832 ratio = 0.844 hyp_len = 94259 ref_len = 111620)\n","chrF2++ = 30.60\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8129/8129 [2:58:08<00:00,  1.31s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n"," ---------------------- Resultados ../results/wayuu/metrica_despues_COMP_NC_3_2e-05.pickle\n","\n","BLEU = 0.87 17.8/2.7/0.5/0.1 (BP = 0.763 ratio = 0.787 hyp_len = 87866 ref_len = 111620)\n","chrF2++ = 20.01\n","Comienza a predecir\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 23/8129 [00:32<3:08:53,  1.40s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m wayuu_pred \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(df_test\u001b[39m.\u001b[39mvalues)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     translated \u001b[39m=\u001b[39m translate(df_test\u001b[39m.\u001b[39mes[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     wayuu_pred\u001b[39m.\u001b[39mappend(translated)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m wayuu_pred2 \u001b[39m=\u001b[39m [w[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m wayuu_pred] \n","\u001b[1;32m/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb Cell 41\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mtgt_lang \u001b[39m=\u001b[39m tgt_lang\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39mmax_input_length)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     forced_bos_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mconvert_tokens_to_ids(tgt_lang),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(a \u001b[39m+\u001b[39m b \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     num_beams\u001b[39m=\u001b[39mnum_beams,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melissa/Documents/NLP/ProyectoNLP/notebooks/revision_resultados_wayuu_fine_tunning_fi.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mbatch_decode(result, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/transformers/generation/utils.py:1752\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1746\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1747\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1748\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1749\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[1;32m   1753\u001b[0m         input_ids,\n\u001b[1;32m   1754\u001b[0m         beam_scorer,\n\u001b[1;32m   1755\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1756\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1757\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1758\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1759\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1760\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1761\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1762\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1763\u001b[0m     )\n\u001b[1;32m   1765\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1766\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/transformers/generation/utils.py:3091\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3089\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3091\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   3092\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   3093\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   3094\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   3095\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   3096\u001b[0m )\n\u001b[1;32m   3098\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3099\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/transformers/models/marian/modeling_marian.py:1402\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1397\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1399\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1400\u001b[0m         )\n\u001b[0;32m-> 1402\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1403\u001b[0m     input_ids,\n\u001b[1;32m   1404\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1405\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1406\u001b[0m     encoder_outputs\u001b[39m=\u001b[39mencoder_outputs,\n\u001b[1;32m   1407\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1408\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m   1409\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1410\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1411\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1412\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1413\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1414\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1415\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1416\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1417\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1418\u001b[0m )\n\u001b[1;32m   1419\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1421\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/transformers/models/marian/modeling_marian.py:1203\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1197\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1198\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1199\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1200\u001b[0m     )\n\u001b[1;32m   1202\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1203\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\n\u001b[1;32m   1204\u001b[0m     input_ids\u001b[39m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1205\u001b[0m     attention_mask\u001b[39m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1206\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1207\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1208\u001b[0m     head_mask\u001b[39m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1209\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1210\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1211\u001b[0m     inputs_embeds\u001b[39m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1212\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1213\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1214\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1215\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1216\u001b[0m )\n\u001b[1;32m   1218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1219\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/transformers/models/marian/modeling_marian.py:1003\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    990\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    991\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    992\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         use_cache,\n\u001b[1;32m   1001\u001b[0m     )\n\u001b[1;32m   1002\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1004\u001b[0m         hidden_states,\n\u001b[1;32m   1005\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1006\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1007\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1008\u001b[0m         layer_head_mask\u001b[39m=\u001b[39m(head_mask[idx] \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1009\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39m(\n\u001b[1;32m   1010\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;00m cross_attn_head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m         ),\n\u001b[1;32m   1012\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m   1013\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1014\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1018\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/transformers/models/marian/modeling_marian.py:432\u001b[0m, in \u001b[0;36mMarianDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    431\u001b[0m cross_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_attn(\n\u001b[1;32m    433\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    434\u001b[0m     key_value_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    435\u001b[0m     attention_mask\u001b[39m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    436\u001b[0m     layer_head_mask\u001b[39m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m    437\u001b[0m     past_key_value\u001b[39m=\u001b[39mcross_attn_past_key_value,\n\u001b[1;32m    438\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    439\u001b[0m )\n\u001b[1;32m    440\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    441\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/transformers/models/marian/modeling_marian.py:167\u001b[0m, in \u001b[0;36mMarianAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    164\u001b[0m bsz, tgt_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[1;32m    166\u001b[0m \u001b[39m# get query proj\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj(hidden_states) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling\n\u001b[1;32m    168\u001b[0m \u001b[39m# get key, value proj\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m# `past_key_value[0].shape[2] == key_value_states.shape[1]`\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m# is checking that the `sequence_length` of the `past_key_value` is the same as\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# the provided `key_value_states` to support prefix tuning\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    173\u001b[0m     is_cross_attention\n\u001b[1;32m    174\u001b[0m     \u001b[39mand\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[39mand\u001b[39;00m past_key_value[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m==\u001b[39m key_value_states\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    176\u001b[0m ):\n\u001b[1;32m    177\u001b[0m     \u001b[39m# reuse k,v, cross_attentions\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/env_NLP/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for model_name in keys_nc:\n","    # Cargar modelo y tokenizados\n","    name = model_name.split('.pickle')[0].replace('metrica_despues', 'modelo')\n","    tokenizer = AutoTokenizer.from_pretrained(name)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(name)\n","\n","    # Traducir\n","    print('Comienza a predecir')\n","    wayuu_pred = []\n","    for i in trange(0, len(df_test.values)):\n","        translated = translate(df_test.es[i])\n","        wayuu_pred.append(translated)\n","    wayuu_pred2 = [w[0] for w in wayuu_pred] \n","\n","    # Resultados \n","    print(f'\\n\\n ---------------------- Resultados {model_name}\\n')\n","    print(bleu_calc.corpus_score(wayuu_pred2, [df_test['fi'].tolist()]))\n","    print(chrf_calc.corpus_score(wayuu_pred2, [df_test['fi'].tolist()]))\n","\n","    # Guardar \n","    name = name.split('/')[-1]\n","    with open(model_path + f'/resultados_traducciones_{name}_wayuu.pickle', 'wb') as file:\n","        pickle.dump(wayuu_pred, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNcGrSalC+OaYa3KcJ7pGwd","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0c91cafc7e9a4d588d51259c9a828816":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e45617308c04c558e54164d106f5f30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9788c66bd5ee4c0f95ae8dbbccc324e5","placeholder":"​","style":"IPY_MODEL_99ec65fa7dc6436f9a5466abb526dc35","value":"100%"}},"43479b3a025d44b087ea8dee0e5a57f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f691bef9bb9a49889e8109837a9c2e3a","placeholder":"​","style":"IPY_MODEL_fc4adf33d874464e97ce682b8a38920a","value":" 35/35 [18:41&lt;00:00, 20.06s/it]"}},"60abb1a1b1e54f29a5ca1c66a82996e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9788c66bd5ee4c0f95ae8dbbccc324e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99ec65fa7dc6436f9a5466abb526dc35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7ac5cf37e2649888c6b2521ddd149e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be70aae8797b487f9d9f7d56fd0875db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60abb1a1b1e54f29a5ca1c66a82996e6","max":35,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7ac5cf37e2649888c6b2521ddd149e7","value":35}},"f691bef9bb9a49889e8109837a9c2e3a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc4adf33d874464e97ce682b8a38920a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fce4c7afab9e47a0a3a401593e9c97e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e45617308c04c558e54164d106f5f30","IPY_MODEL_be70aae8797b487f9d9f7d56fd0875db","IPY_MODEL_43479b3a025d44b087ea8dee0e5a57f8"],"layout":"IPY_MODEL_0c91cafc7e9a4d588d51259c9a828816"}}}}},"nbformat":4,"nbformat_minor":0}
